{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7BZejKDpcAL"
      },
      "source": [
        "# Android Security Bulletin Data Download\n",
        "\n",
        "This notebook downloads data on CVEs reported on the monthly Android Security Bulletin, and merges it with the data provided by the CVE Program and with the data found on the AOSP git repo. The resulting aggregated data contains information about the dates of events of interest (e.g. commit to the repo, publication on the bulletin) for every patch published on the bulletin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7b7s7_lVIXT"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "import dateutil\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "from os import path, listdir\n",
        "\n",
        "\n",
        "BASE_URL = \"https://source.android.com\"\n",
        "LIST_URL = \"https://source.android.com/security/bulletin\"\n",
        "CVE_REGEX = re.compile(\"CVE-\\d+-\\d+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dateutil import parser\n",
        "def get_url_and_date(url_page):\n",
        "  ret = []\n",
        "  page = requests.get(url_page)\n",
        "  doc = BeautifulSoup(page.content, \"html.parser\")\n",
        "  rows = doc.find_all(\"tr\")\n",
        "  for row in rows[1:]:\n",
        "    url = BASE_URL + row.find(\"td\").find(\"a\")[\"href\"]\n",
        "    date = parser.parse(row.find_all(\"td\")[2].get_text().strip())\n",
        "    ret.append((url, date))\n",
        "  return ret\n",
        "\n",
        "urls = get_url_and_date(LIST_URL)\n",
        "\n",
        "# filter urls\n",
        "urls = [ (u, d) for u, d in urls if path.basename(u) < \"2023\" and path.basename(u) >= \"2017-06-01\"]\n",
        "urls"
      ],
      "metadata": {
        "id": "NH8pKdP-F2Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPZjLVZ9CIJX"
      },
      "source": [
        "! wget https://cve.mitre.org/data/downloads/allitems-cvrf.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOSa1G0ZxFoX"
      },
      "source": [
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.gz\n",
        "! wget https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2022.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2021.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2020.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2019.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2018.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2017.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2016.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2015.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2014.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2013.json.gz\n",
        "! gzip -dkf nvdcve-1.1-2012.json.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIqu98vZUsDK"
      },
      "source": [
        "ns = {\n",
        "    \"vuln\": \"http://www.icasi.org/CVRF/schema/vuln/1.1\",\n",
        "    \"cvrf\": \"http://www.icasi.org/CVRF/schema/cvrf/1.1\",\n",
        "}\n",
        "\n",
        "def load_cvrf(filename):\n",
        "  tree = ET.parse(filename)\n",
        "  doc = tree.getroot()\n",
        "  vulns = doc.findall(\"vuln:Vulnerability\", namespaces=ns)\n",
        "  ret = []\n",
        "  for vuln in vulns:\n",
        "    name = vuln.find(\"vuln:Title\", namespaces=ns).text\n",
        "    published = vuln.find(\"./vuln:Notes/vuln:Note[@Title='Published']\", namespaces=ns)\n",
        "    if published is not None:\n",
        "      published = published.text\n",
        "    modified = vuln.find(\"./vuln:Notes/vuln:Note[@Title='Modified']\", namespaces=ns)\n",
        "    if modified is not None:\n",
        "      modified = modified.text\n",
        "    ret.append({\"cve\": name, \"published\": published, \"modified\": modified})\n",
        "  return pd.DataFrame(ret)\n",
        "\n",
        "def get_modified(df, id):\n",
        "  return df[df[\"cve\"] == id].iloc[0].at[\"modified\"]\n",
        "\n",
        "def get_published(df, id):\n",
        "  return df[df[\"cve\"] == id].iloc[0].at[\"published\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHpCu6E6BhHi"
      },
      "source": [
        "# TODO: use this to filter Acknoledgements table?\n",
        "def filter_table(table):\n",
        "  cur = table.previous_sibling\n",
        "  while cur.name != \"h2\":\n",
        "    cur = cur.previous_sibling\n",
        "  return cur.get_text().strip().lower() == \"acknowledgements\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slucJA5QcOfy"
      },
      "source": [
        "def print_rows(rows):\n",
        "  for row in rows:\n",
        "    print(row.get_text())\n",
        "\n",
        "def filter_rows(rows):\n",
        "  return [row for row in rows if CVE_REGEX.search(row.get_text()) is not None]\n",
        "\n",
        "def get_cves(info, get_commits=True):\n",
        "  url, bulletin_date = info\n",
        "  cves = []\n",
        "  patch_level = path.basename(url)\n",
        "  page = requests.get(url)\n",
        "  doc = BeautifulSoup(page.content, \"html.parser\")\n",
        "  tables = doc.find_all(\"table\")\n",
        "  #tables = [t for t in tables if not filter_table(t)]\n",
        "  for table in tables:\n",
        "    category = table.find_previous(\"h3\").get_text().strip().lower()\n",
        "    rows = filter_rows(table.find_all(\"td\"))\n",
        "    rows2 = table.find_all(\"td\", string=CVE_REGEX)\n",
        "    for row in rows:\n",
        "      cells = row.parent.find_all(\"td\")\n",
        "      if category.startswith(\"google play\"):\n",
        "        cve = cells[1].get_text().strip()\n",
        "        print(cve)\n",
        "      else:\n",
        "        cve = cells[0].get_text().strip()\n",
        "      refs = []\n",
        "      if get_commits:\n",
        "        links = cells[1].find_all(\"a\")\n",
        "        if links is not None and CVE_REGEX.search(cve) is not None:\n",
        "          for a in links:\n",
        "            try:\n",
        "              link = a[\"href\"]\n",
        "              (created, committed) = get_commit_date(link)\n",
        "              ref = {\"link\": link, \"created\": created, \"committed\": committed}\n",
        "              refs.append(ref)\n",
        "            except:\n",
        "              pass\n",
        "      cur_cves = cve.split(',')\n",
        "      for c in cur_cves:\n",
        "        cves.append({\"cve\": c.strip(), \"refs\": refs, \"bulletin_date\": bulletin_date, \"category\": category, \"patch_level\": patch_level })\n",
        "  return cves\n",
        "\n",
        "def get_date_from_row(doc, head):\n",
        "  try:\n",
        "    row = doc.find(\"th\", string=head).parent.find_all(\"td\")\n",
        "    return dateutil.parser.parse(row[1].text)\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "# matches codeaurora urls, making a distiction between cgit urls and gitweb urls.\n",
        "# in case an url is a cgit url (i.e. group 1 matches), group 2 contains the path\n",
        "# of the repo and group 3 contains the commit id.\n",
        "# In case an url is a gitweb url, group 4 contains the path (including a '?p=...'\n",
        "# parameter that shouuld be removed) and group 5 contains the commit id\n",
        "CODEAURORA_RE = re.compile(r\"https:\\/\\/(source\\.codeaurora\\.org|us\\.codeaurora\\.org\\/cgit)\\/quic\\/(.*)\\/commit\\/?\\?.*id=(.*)|https:\\/\\/www\\.codeaurora\\.org\\/gitweb\\/quic\\/(.*\\/?p=.*)\\.git;a=commit;h=(.*)\")\n",
        "\n",
        "def fix_codeaurora_url(url):\n",
        "  matches = CODEAURORA_RE.match(url)\n",
        "  if matches:\n",
        "    if matches.group(1):\n",
        "      path = matches.group(2)\n",
        "      commit = matches.group(3)\n",
        "    else:\n",
        "      path = matches.group(4).replace('?p=', '')\n",
        "      commit = matches.group(5)\n",
        "    url = f\"https://git.codelinaro.org/clo/{path}/-/commit/{commit}\"\n",
        "  return url\n",
        "\n",
        "# github web pages do not have all the information we can extract from cgit, so\n",
        "# we can use the url for the old repo\n",
        "def fix_github_url(url):\n",
        "  return url.replace(\"github.com/torvalds/linux/commit/\", \"git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=\")\n",
        "\n",
        "def fix_url(url):\n",
        "  url = fix_codeaurora_url(url)\n",
        "  url = fix_github_url(url)\n",
        "  return url\n",
        "\n",
        "def get_codelinaro_date(doc):\n",
        "  try:\n",
        "    times = doc.findAll(\"time\")\n",
        "    return (dateutil.parser.parse(times[0][\"datetime\"]), dateutil.parser.parse(times[1][\"datetime\"]))\n",
        "  except:\n",
        "    return (None, None)\n",
        "\n",
        "def get_commit_date(url):\n",
        "  url = fix_url(url)\n",
        "  print(f\"\\tget commit date from {url}...\")\n",
        "  page = requests.get(url)\n",
        "  doc = BeautifulSoup(page.content, \"html.parser\")\n",
        "  if \"git.codelinaro.org\" in url:\n",
        "    return get_codelinaro_date(doc)\n",
        "  return (get_date_from_row(doc, \"author\"), get_date_from_row(doc, \"committer\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHSVMGzS6Y5S"
      },
      "source": [
        "cves = []\n",
        "for url in set(urls):\n",
        "  print(f\"download {url}...\")\n",
        "  cves.extend(get_cves(url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(data=cves)\n",
        "df_cat = df[df[\"bulletin_date\"] >= \"2017-06-01\"]\n",
        "df_cat.groupby(\"category\")[\"bulletin_date\"].count()"
      ],
      "metadata": {
        "id": "CbMJgSm7v_yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvUU0pQ0yUvj"
      },
      "source": [
        "def load_nvd(filename):\n",
        "  data = dict()\n",
        "  with open(filename, \"r\") as f:\n",
        "    payload = json.load(f)\n",
        "    for item in payload[\"CVE_Items\"]:\n",
        "      id = item[\"cve\"][\"CVE_data_meta\"][\"ID\"]\n",
        "      data[id] = item\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18NByXsJmVJT"
      },
      "source": [
        "nvd_files = [ f for f in listdir() if path.isfile(f) and f.startswith(\"nvdcve\") and f.endswith(\".json\")]\n",
        "\n",
        "nvds = dict()\n",
        "cve_meta = load_cvrf(\"allitems-cvrf.xml\")\n",
        "\n",
        "for filename in nvd_files:\n",
        "  print(f\"loading {filename}\")\n",
        "  nvds = {**nvds, **load_nvd(filename)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_remove = []\n",
        "\n",
        "for cve in cves:\n",
        "  id = cve[\"cve\"]\n",
        "  try:\n",
        "    mod = get_modified(cve_meta, id)\n",
        "    pub = get_published(cve_meta, id)\n",
        "    cve[\"cve_modified\"] = mod\n",
        "    cve[\"cve_published\"] = pub\n",
        "    if id in nvds and nvds[id][\"impact\"]:\n",
        "      if \"baseMetricV2\" in nvds[id][\"impact\"]:\n",
        "        impact = nvds[id][\"impact\"][\"baseMetricV2\"]\n",
        "        score = impact[\"cvssV2\"][\"baseScore\"]\n",
        "        severity = impact[\"severity\"]\n",
        "        cve[\"nvd_severity\"] = severity\n",
        "        cve[\"nvd_score\"] = score\n",
        "      if \"baseMetricV3\" in nvds[id][\"impact\"]:\n",
        "        impact = nvds[id][\"impact\"][\"baseMetricV3\"]\n",
        "        cve[\"nvd_score_v3\"] = impact[\"cvssV3\"][\"baseScore\"]\n",
        "        cve[\"nvd_severity_v3\"] = impact[\"cvssV3\"][\"baseSeverity\"]\n",
        "    else:\n",
        "      cve[\"nvd_severity\"] = None\n",
        "      cve[\"nvd_score\"] = None\n",
        "      print(f\"skipping NVD for {id}...\")\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(f\"error with cve {id}, skipping\")\n",
        "    to_remove.append(cve)\n",
        "\n",
        "for cve in to_remove:\n",
        "  cves.remove(cve)"
      ],
      "metadata": {
        "id": "i8aSkaC9iU2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE_k2ztV7Rh2"
      },
      "source": [
        "output = \"drive/My Drive/cves_temp/cves-severity-google-play.json\"\n",
        "\n",
        "with open(output, \"w\") as f:\n",
        "  json.dump(cves, f, default=str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H030v10yMryf"
      },
      "source": [
        "import csv\n",
        "\n",
        "def to_csv(cves, out):\n",
        "  \"\"\"\n",
        "  Converts the raw cves to a csv file. The various commit dates are aggregated into\n",
        "  `commit_start` and `commit_end`, which are respectively the earliest and the\n",
        "  latest dates that appear in the various commits.\n",
        "  Duplicated CVEs are left as is, and should be handled in another preprocessing \n",
        "  step\n",
        "  \"\"\"\n",
        "  with open(out, \"w\") as csvfile:\n",
        "    w = csv.writer(csvfile)\n",
        "    w.writerow([\"id\", \"bulletin_date\", \"patch_level\", \"category\", \"published\", \"modified\", \"commit_start\", \"commit_end\", \"nvd_severity\", \"nvd_score\", \"nvd_severity_v3\", \"nvd_score_v3\",])\n",
        "    for cve in cves:\n",
        "      dates = [d for r in cve[\"refs\"] for d in [r[\"committed\"], r[\"created\"]] if r[\"committed\"] is not None and r[\"created\"] is not None]\n",
        "      if dates:\n",
        "        start = min(dates)\n",
        "        end = max(dates)\n",
        "      else:\n",
        "        start = None\n",
        "        end = None\n",
        "      try:\n",
        "        w.writerow([cve[\"cve\"], cve[\"bulletin_date\"], cve[\"patch_level\"], cve.get(\"category\"), cve.get(\"cve_published\"), cve.get(\"cve_modified\"), start, end, cve.get(\"nvd_severity\"), cve.get(\"nvd_score\"), cve.get(\"nvd_severity_v3\"), cve.get(\"nvd_score_v3\")])\n",
        "      except Exception as e:\n",
        "        print(f\"error {e} with cve {cve['cve']}, skipping...\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MfpFdClSpj4"
      },
      "source": [
        "output = \"drive/My Drive/cves_temp/aggregated-severity-cat-google-play.csv\"\n",
        "\n",
        "to_csv(cves, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "output = \"drive/My Drive/cves_temp/bulletin_dates.csv\"\n",
        "\n",
        "bulletin_dates = [(path.basename(url), date.strftime(\"%Y-%m-%d\")) for url, date in urls]\n",
        "\n",
        "with open(output, \"w\") as csvfile:\n",
        "  w = csv.writer(csvfile)\n",
        "  w.writerow([\"security_patch\", \"date\"])\n",
        "  for row in bulletin_dates:\n",
        "    w.writerow(row)\n"
      ],
      "metadata": {
        "id": "0Y3L6QH8JjjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}